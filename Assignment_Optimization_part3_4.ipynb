{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer # 3**\n",
    "\n",
    "***Gradient Operator***\n",
    "The gradeint operator is aslo called as Hamilton operator, it is a vector operator for any N-dimesnsional function. \n",
    "Its takes the the function derivatives with repect to given variable and put it in a single matrix. \n",
    "It is also denoted by ∇ (the nabla symbol).\n",
    "\n",
    "∇f(x,y) = [f'(x)\n",
    "            f'(y)]\n",
    "            \n",
    "            \n",
    "***Eigenfunction & Eigenvalues***\n",
    "\n",
    "An eigenfunction of a linear operator 'D' defined on some function space is any non-zero function 'f' in that space that, when acted upon by D, is only multiplied by some scalling factor called eigenvalue. \n",
    "\n",
    "equation:\n",
    "Df = λf\n",
    "\n",
    "            \n",
    "\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "          \n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "**Answer # 4**\n",
    "**Hessian related to the gradient**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Derivative***\n",
    "\n",
    "Derivattive can be defined as the rate of change of any dependent variable (let say)'y' with respect to any independent variable (let say)'x\n",
    "\n",
    "mathematically, it can be shown as : dy / dx\n",
    "\n",
    "\n",
    "***Gradient***\n",
    "\n",
    "It is the rate of changes of some function (in deep learning, this is generally the loss function) in various directions. A gradient is simply a collection of the derivatives of the function for each direction. Each element of the gradient is simply the slope of the function.\n",
    "\n",
    "***Hessian***\n",
    "It is basically the derivative of the Gradient. Technically, it is the secondary derivative of the changing rate of slope. Furthermore, If the eigenvalues are larger then the Gradient is also larger. When the eigenvalues are larger then there is a larger curvature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
